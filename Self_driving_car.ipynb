{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Driving Car "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XHFnthirwlfn"
   },
   "outputs": [],
   "source": [
    "# Credits: https://github.com/SullyChen/Autopilot-TensorFlow\n",
    "# Research paper: End to End Learning for Self-Driving Cars by Nvidia. [https://arxiv.org/pdf/1604.07316.pdf]\n",
    "\n",
    "# NVidia dataset: 72 hrs of video => 72*60*60*30 = 7,776,000 images\n",
    "# Nvidia blog: https://devblogs.nvidia.com/deep-learning-self-driving-cars/\n",
    "\n",
    "\n",
    "# Our Dataset: https://github.com/SullyChen/Autopilot-TensorFlow [https://drive.google.com/file/d/0B-KJCaaF7elleG1RbzVPZWV4Tlk/view]\n",
    "# Size: 25 minutes = 25*60*30 = 45,000 images ~ 2.3 GB\n",
    "\n",
    "\n",
    "# If you want to try on a slightly large dataset: 70 minutes of data ~ 223GB\n",
    "# Refer: https://medium.com/udacity/open-sourcing-223gb-of-mountain-view-driving-data-f6b5593fbfa5\n",
    "# Format: Image, latitude, longitude, gear, brake, throttle, steering angles and speed\n",
    "\n",
    "\n",
    "\n",
    "# Additional Installations:\n",
    "# pip3 install h5py\n",
    "\n",
    "\n",
    "# AWS: https://aws.amazon.com/blogs/machine-learning/get-started-with-deep-learning-using-the-aws-deep-learning-ami/\n",
    "\n",
    "# Youtube:https://www.youtube.com/watch?v=qhUvQiKec2U\n",
    "# Further reading and extensions: https://medium.com/udacity/teaching-a-machine-to-steer-a-car-d73217f2492c\n",
    "# More data: https://medium.com/udacity/open-sourcing-223gb-of-mountain-view-driving-data-f6b5593fbfa5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "vgcc6iQobKHi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing data.txt\n"
     ]
    }
   ],
   "source": [
    "# read images and steering angles from driving_dataset folder\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from scipy import pi\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "\n",
    "DATA_FOLDER = './driving_dataset/' # change this to your folder\n",
    "TRAIN_FILE = os.path.join(DATA_FOLDER, 'data.txt')\n",
    "\n",
    "\n",
    "split =0.7\n",
    "X = []\n",
    "y = []\n",
    "with open(TRAIN_FILE) as fp:\n",
    "    for line in islice(fp, 45406):\n",
    "        path, angle = line.strip().split()\n",
    "        full_path = os.path.join(DATA_FOLDER, path)\n",
    "        X.append(full_path)\n",
    "        \n",
    "        # converting angle from degrees to radians\n",
    "        y.append(float(angle) * pi / 180 )\n",
    "\n",
    "\n",
    "y = np.array(y)\n",
    "print(\"Completed processing data.txt\")\n",
    "\n",
    "split_index = int(len(y)*0.7)\n",
    "\n",
    "train_y = y[:split_index]\n",
    "test_y = y[split_index:]\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy;\n",
    "\n",
    "# PDF of train and test 'y' values. \n",
    "import matplotlib.pyplot as plt \n",
    "plt.hist(train_y, bins=50, normed=1, color='green', histtype ='step');\n",
    "plt.hist(test_y, bins=50, normed=1, color='red', histtype ='step');\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_MSE(MEAN):0.241561\n",
      "Test_MSE(ZERO):0.241107\n"
     ]
    }
   ],
   "source": [
    "#Model 0: Base line Model: y_test_pred = mean(y_train_i) \n",
    "train_mean_y = np.mean(train_y)\n",
    "\n",
    "print('Test_MSE(MEAN):%f' % np.mean(np.square(test_y-train_mean_y)) )\n",
    "\n",
    "print('Test_MSE(ZERO):%f' % np.mean(np.square(test_y-0.0)) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Loading and Transforming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.misc\n",
    "import random\n",
    "\n",
    "xs = []\n",
    "ys = []\n",
    "\n",
    "#points to the end of the last batch\n",
    "train_batch_pointer = 0\n",
    "val_batch_pointer = 0\n",
    "\n",
    "#read data.txt\n",
    "with open(\"driving_dataset/data.txt\") as f:\n",
    "    for line in f:\n",
    "        xs.append(\"driving_dataset/\" + line.split()[0])\n",
    "        #the paper by Nvidia uses the inverse of the turning radius,\n",
    "        #but steering wheel angle is proportional to the inverse of turning radius\n",
    "        #so the steering wheel angle in radians is used as the output\n",
    "        ys.append(float(line.split()[1]) * scipy.pi / 180)\n",
    "\n",
    "#get number of images\n",
    "num_images = len(xs)\n",
    "\n",
    "\n",
    "train_xs = xs[:int(len(xs) * 0.7)]\n",
    "train_ys = ys[:int(len(xs) * 0.7)]\n",
    "\n",
    "val_xs = xs[-int(len(xs) * 0.3):]\n",
    "val_ys = ys[-int(len(xs) * 0.3):]\n",
    "\n",
    "num_train_images = len(train_xs)\n",
    "num_val_images = len(val_xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadTrainBatch(batch_size):\n",
    "    global train_batch_pointer\n",
    "    x_out = []\n",
    "    y_out = []\n",
    "    for i in range(0, batch_size):\n",
    "        x_out.append(scipy.misc.imresize(scipy.misc.imread(train_xs[(train_batch_pointer + i) % \\\n",
    "                                                                    num_train_images])[-150:],[66, 200]) /255.0)\n",
    "        y_out.append([train_ys[(train_batch_pointer + i) % num_train_images]])\n",
    "    train_batch_pointer += batch_size\n",
    "    return x_out, y_out\n",
    "\n",
    "def LoadValBatch(batch_size):\n",
    "    global val_batch_pointer\n",
    "    x_out = []\n",
    "    y_out = []\n",
    "    for i in range(0, batch_size):\n",
    "        x_out.append(scipy.misc.imresize(scipy.misc.imread(val_xs[(val_batch_pointer + i) % \\\n",
    "                                                                  num_val_images])[-150:], [66, 200]) / 255.0)\n",
    "        y_out.append([val_ys[(val_batch_pointer + i) % num_val_images]])\n",
    "    val_batch_pointer += batch_size\n",
    "    return x_out, y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import scipy\n",
    "\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, stride):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding='VALID')\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 66, 200, 3])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "x_image = x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first convolutional layer\n",
    "W_conv1 = weight_variable([5, 5, 3, 24])\n",
    "b_conv1 = bias_variable([24])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1, 2) + b_conv1)\n",
    "\n",
    "#second convolutional layer\n",
    "W_conv2 = weight_variable([5, 5, 24, 36])\n",
    "b_conv2 = bias_variable([36])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_conv1, W_conv2, 2) + b_conv2)\n",
    "\n",
    "#third convolutional layer\n",
    "W_conv3 = weight_variable([5, 5, 36, 48])\n",
    "b_conv3 = bias_variable([48])\n",
    "\n",
    "h_conv3 = tf.nn.relu(conv2d(h_conv2, W_conv3, 2) + b_conv3)\n",
    "\n",
    "#fourth convolutional layer\n",
    "W_conv4 = weight_variable([3, 3, 48, 64])\n",
    "b_conv4 = bias_variable([64])\n",
    "\n",
    "h_conv4 = tf.nn.relu(conv2d(h_conv3, W_conv4, 1) + b_conv4)\n",
    "\n",
    "#fifth convolutional layer\n",
    "W_conv5 = weight_variable([3, 3, 64, 64])\n",
    "b_conv5 = bias_variable([64])\n",
    "\n",
    "h_conv5 = tf.nn.relu(conv2d(h_conv4, W_conv5, 1) + b_conv5)\n",
    "\n",
    "#FCL 1\n",
    "W_fc1 = weight_variable([1152, 1164])\n",
    "b_fc1 = bias_variable([1164])\n",
    "\n",
    "h_conv5_flat = tf.reshape(h_conv5, [-1, 1152])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_conv5_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "#FCL 2\n",
    "W_fc2 = weight_variable([1164, 100])\n",
    "b_fc2 = bias_variable([100])\n",
    "\n",
    "h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "\n",
    "#FCL 3\n",
    "W_fc3 = weight_variable([100, 50])\n",
    "b_fc3 = bias_variable([50])\n",
    "\n",
    "h_fc3 = tf.nn.relu(tf.matmul(h_fc2_drop, W_fc3) + b_fc3)\n",
    "\n",
    "h_fc3_drop = tf.nn.dropout(h_fc3, keep_prob)\n",
    "\n",
    "#FCL 3\n",
    "W_fc4 = weight_variable([50, 10])\n",
    "b_fc4 = bias_variable([10])\n",
    "\n",
    "h_fc4 = tf.nn.relu(tf.matmul(h_fc3_drop, W_fc4) + b_fc4)\n",
    "\n",
    "h_fc4_drop = tf.nn.dropout(h_fc4, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output\n",
    "W_fc5 = weight_variable([10, 1])\n",
    "b_fc5 = bias_variable([1])\n",
    "\n",
    "y =tf.matmul(h_fc4_drop, W_fc5) + b_fc5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.core.protobuf import saver_pb2\n",
    "import driving_data\n",
    "import model\n",
    "\n",
    "LOGDIR = './save'\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "L2NormConst = 0.001\n",
    "\n",
    "train_vars = tf.trainable_variables()\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(tf.subtract(model.y_, model.y))) + \\\n",
    "tf.add_n([tf.nn.l2_loss(v) for v in train_vars]) * L2NormConst\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "# create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "# merge all summaries into a single op\n",
    "merged_summary_op =  tf.summary.merge_all()\n",
    "\n",
    "saver = tf.train.Saver(write_version = saver_pb2.SaverDef.V1)\n",
    "\n",
    "# op to write logs to Tensorboard\n",
    "logs_path = './logs'\n",
    "summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train over the dataset about 30 times\n",
    "for epoch in range(epochs):\n",
    "  for i in range(int(driving_data.num_images/batch_size)):\n",
    "    xs, ys = driving_data.LoadTrainBatch(batch_size)\n",
    "    train_step.run(feed_dict={model.x: xs, model.y_: ys, model.keep_prob: 0.5})\n",
    "    if i % 10 == 0:\n",
    "      xs, ys = driving_data.LoadValBatch(batch_size)\n",
    "      loss_value = loss.eval(feed_dict={model.x:xs, model.y_: ys, model.keep_prob: 1.0})\n",
    "      print(\"Epoch: %d, Step: %d, Loss: %g\" % (epoch, epoch * batch_size + i, loss_value))\n",
    "\n",
    "    # write logs at every iteration\n",
    "    summary = merged_summary_op.eval(feed_dict={model.x:xs, model.y_: ys, model.keep_prob: 1.0})\n",
    "    summary_writer.add_summary(summary, epoch * driving_data.num_images/batch_size + i)\n",
    "\n",
    "    if i % batch_size == 0:\n",
    "      if not os.path.exists(LOGDIR):\n",
    "        os.makedirs(LOGDIR)\n",
    "      checkpoint_path = os.path.join(LOGDIR, \"model.ckpt\")\n",
    "      filename = saver.save(sess, checkpoint_path)\n",
    "  print(\"Model saved in file: %s\" % filename)\n",
    "\n",
    "print(\"Run the command line:\\n\" \\\n",
    "          \"--> tensorboard --logdir=./logs \" \\\n",
    "          \"\\nThen open http://0.0.0.0:6006/ into your web browser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminal Output while execution (only last epoch o/p is shown here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Epoch: 29, Step: 2900, Loss: 0.160061\n",
    "WARNING:tensorflow:*******************************************************\n",
    "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
    "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
    "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
    "WARNING:tensorflow:now on by default.\n",
    "WARNING:tensorflow:*******************************************************\n",
    "Epoch: 29, Step: 2910, Loss: 0.165028\n",
    "Epoch: 29, Step: 2920, Loss: 0.122759\n",
    "Epoch: 29, Step: 2930, Loss: 0.145727\n",
    "Epoch: 29, Step: 2940, Loss: 0.135195\n",
    "Epoch: 29, Step: 2950, Loss: 0.131849\n",
    "Epoch: 29, Step: 2960, Loss: 0.117785\n",
    "Epoch: 29, Step: 2970, Loss: 0.129923\n",
    "Epoch: 29, Step: 2980, Loss: 0.121361\n",
    "Epoch: 29, Step: 2990, Loss: 0.112543\n",
    "Epoch: 29, Step: 3000, Loss: 0.202535\n",
    "WARNING:tensorflow:*******************************************************\n",
    "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
    "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
    "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
    "WARNING:tensorflow:now on by default.\n",
    "WARNING:tensorflow:*******************************************************\n",
    "Epoch: 29, Step: 3010, Loss: 0.138245\n",
    "Epoch: 29, Step: 3020, Loss: 0.156304\n",
    "Epoch: 29, Step: 3030, Loss: 0.130026\n",
    "Epoch: 29, Step: 3040, Loss: 0.227233\n",
    "Epoch: 29, Step: 3050, Loss: 0.141504\n",
    "Epoch: 29, Step: 3060, Loss: 0.123712\n",
    "Epoch: 29, Step: 3070, Loss: 0.132455\n",
    "Epoch: 29, Step: 3080, Loss: 0.149798\n",
    "Epoch: 29, Step: 3090, Loss: 0.161697\n",
    "Epoch: 29, Step: 3100, Loss: 0.145025\n",
    "WARNING:tensorflow:*******************************************************\n",
    "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
    "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
    "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
    "WARNING:tensorflow:now on by default.\n",
    "WARNING:tensorflow:*******************************************************\n",
    "Epoch: 29, Step: 3110, Loss: 0.151707\n",
    "Epoch: 29, Step: 3120, Loss: 0.119785\n",
    "Epoch: 29, Step: 3130, Loss: 0.117604\n",
    "Epoch: 29, Step: 3140, Loss: 0.129332\n",
    "Epoch: 29, Step: 3150, Loss: 0.122629\n",
    "Epoch: 29, Step: 3160, Loss: 0.136966\n",
    "Epoch: 29, Step: 3170, Loss: 0.124979\n",
    "Epoch: 29, Step: 3180, Loss: 0.550006\n",
    "Epoch: 29, Step: 3190, Loss: 0.113502\n",
    "Epoch: 29, Step: 3200, Loss: 0.150389\n",
    "WARNING:tensorflow:*******************************************************\n",
    "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
    "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
    "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
    "WARNING:tensorflow:now on by default.\n",
    "WARNING:tensorflow:*******************************************************\n",
    "Epoch: 29, Step: 3210, Loss: 0.165526\n",
    "Epoch: 29, Step: 3220, Loss: 0.417433\n",
    "Epoch: 29, Step: 3230, Loss: 0.16974\n",
    "Epoch: 29, Step: 3240, Loss: 0.121649\n",
    "Epoch: 29, Step: 3250, Loss: 0.125182\n",
    "Epoch: 29, Step: 3260, Loss: 0.214315\n",
    "Epoch: 29, Step: 3270, Loss: 0.134367\n",
    "Epoch: 29, Step: 3280, Loss: 0.148181\n",
    "Epoch: 29, Step: 3290, Loss: 0.110607\n",
    "Epoch: 29, Step: 3300, Loss: 0.126141\n",
    "WARNING:tensorflow:*******************************************************\n",
    "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
    "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
    "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
    "WARNING:tensorflow:now on by default.\n",
    "WARNING:tensorflow:*******************************************************\n",
    "Epoch: 29, Step: 3310, Loss: 0.152955\n",
    "Epoch: 29, Step: 3320, Loss: 0.506155\n",
    "Epoch: 29, Step: 3330, Loss: 0.280343\n",
    "Epoch: 29, Step: 3340, Loss: 0.536558\n",
    "Epoch: 29, Step: 3350, Loss: 0.195544\n",
    "Model saved in file: ./save/model.ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation and Steps followed to do this case study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model choosen here is Convolutional Neural Networks (CNN) to predict the angle of steering. The model is trained for 30 epochs and the final loss observed after 30 epochs is 0.195544. For each epoch a batch size of 100 is choosen. The loss at the first epoch was very high compared to the final loss reported. When executed, the steering was rotating in the direction of movement of the car.\n",
    "\n",
    "The entire dataset is split into 70% and 30% as train and test data sets. The output function here is a linear function instead of atan function. But the result viewd for both are not very different. The learning rate for Adam optimizer is choosen as 1e-4 and the dropout rate is changed from 0.8 to 0.5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Steps__\n",
    "\n",
    "1. The dataset is available as a series of images which or obtained from a video as for each second 30 frames are there and 30 images can be obtained. Along with this each image has its corresponding steering angle which is stored in a separate file. Divide this set tin 70:30 ratio for training and testing. Already this dataset is in time order. Data has to be loaded in batches. So define functions for doing this both for train data and vallidation data.\n",
    "2. In this step we define the CNN model. This model consists of 5 layers with ReLU activation funcion. Weight vector and bias is calculated at every layer and feeded to the next layer. Final output funcion is a linear funcion.\n",
    "3. After defining the model, we need to compile and train it.The model is optimised with AdamOptimizer with learning rate of 1e-4. The dropout rate used is 0.5. While training for each image its steering angle is predicted and and stored for computing the loss of that batch. Finally the loss is reported. To get a good visual output, each input image, its predicted steering angle and a steering image is taken and this image is tried to rotate the predicted steering angle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entier output produced at terminal is copied to a text file manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Self_driving_car.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
